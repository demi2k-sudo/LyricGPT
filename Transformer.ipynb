{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5589bca6",
   "metadata": {},
   "source": [
    "A small level language model based on the Transformer architecture that is trained on Spotify songs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b30980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2481dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456447ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1be029",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "block_size = 64\n",
    "learning_rate = 3e-4\n",
    "# n_embd = 384\n",
    "# n_head = 6\n",
    "# n_layers = 6\n",
    "# dropout = 0.2\n",
    "# batch_size = 64\n",
    "# block_size = 256\n",
    "# learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5a7e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  68196771\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635dd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text[:1115394] # Andrej's shakespeare dataset was of this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4f1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  68196771\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9480b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Look at her face, it's a wonderful face  \n",
      "And it means something special to me  \n",
      "Look at the way that she smiles when she sees me  \n",
      "How lucky can one fellow be?  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, without her I'm blue  \n",
      "And if she ever leaves me what could I do, what could I do?  \n",
      "  \n",
      "And when we go for a walk in the park  \n",
      "And she holds me and squeezes my hand  \n",
      "We'll go on walking for hours and talking  \n",
      "About all the things that we plan  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, without her I'm blue  \n",
      "And if she ever leaves me what could I do, what could I do?\n",
      "\n",
      "\"\n",
      "\"Take it easy with me, please  \n",
      "Touch me gently like a summer evening breeze  \n",
      "Take your time, make it slow  \n",
      "Andante, Andante  \n",
      "Just let the feeling grow  \n",
      "  \n",
      "Make your fingers soft and light  \n",
      "Let your body be the velvet of the night \n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0de4ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),-.0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7dac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i.item()] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# print(encode(\"hii there\"))\n",
    "# print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbaa3c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68196771]) torch.int64\n",
      "tensor([ 3, 33, 64, 64, 60,  1, 50, 69,  1, 57, 54, 67,  1, 55, 50, 52, 54,  7,\n",
      "         1, 58, 69,  4, 68,  1, 50,  1, 72, 64, 63, 53, 54, 67, 55, 70, 61,  1,\n",
      "        55, 50, 52, 54,  1,  1,  0, 22, 63, 53,  1, 58, 69,  1, 62, 54, 50, 63,\n",
      "        68,  1, 68, 64, 62, 54, 69, 57, 58, 63, 56,  1, 68, 65, 54, 52, 58, 50,\n",
      "        61,  1, 69, 64,  1, 62, 54,  1,  1,  0, 33, 64, 64, 60,  1, 50, 69,  1,\n",
      "        69, 57, 54,  1, 72, 50, 74,  1, 69, 57, 50, 69,  1, 68, 57, 54,  1, 68,\n",
      "        62, 58, 61, 54, 68,  1, 72, 57, 54, 63,  1, 68, 57, 54,  1, 68, 54, 54,\n",
      "        68,  1, 62, 54,  1,  1,  0, 29, 64, 72,  1, 61, 70, 52, 60, 74,  1, 52,\n",
      "        50, 63,  1, 64, 63, 54,  1, 55, 54, 61, 61, 64, 72,  1, 51, 54, 21,  1,\n",
      "         1,  0,  1,  1,  0, 40, 57, 54,  4, 68,  1, 59, 70, 68, 69,  1, 62, 74,\n",
      "         1, 60, 58, 63, 53,  1, 64, 55,  1, 56, 58, 67, 61,  7,  1, 68, 57, 54,\n",
      "         1, 62, 50, 60, 54, 68,  1, 62, 54,  1, 55, 54, 54, 61,  1, 55, 58, 63,\n",
      "        54,  1,  1,  0, 44, 57, 64,  1, 52, 64, 70, 61, 53,  1, 54, 71, 54, 67,\n",
      "         1, 51, 54, 61, 58, 54, 71, 54,  1, 69, 57, 50, 69,  1, 68, 57, 54,  1,\n",
      "        52, 64, 70, 61, 53,  1, 51, 54,  1, 62, 58, 63, 54, 21,  1,  1,  0, 40,\n",
      "        57, 54,  4, 68,  1, 59, 70, 68, 69,  1, 62, 74,  1, 60, 58, 63, 53,  1,\n",
      "        64, 55,  1, 56, 58, 67, 61,  7,  1, 72, 58, 69, 57, 64, 70, 69,  1, 57,\n",
      "        54, 67,  1, 30,  4, 62,  1, 51, 61, 70, 54,  1,  1,  0, 22, 63, 53,  1,\n",
      "        58, 55,  1, 68, 57, 54,  1, 54, 71, 54, 67,  1, 61, 54, 50, 71, 54, 68,\n",
      "         1, 62, 54,  1, 72, 57, 50, 69,  1, 52, 64, 70, 61, 53,  1, 30,  1, 53,\n",
      "        64,  7,  1, 72, 57, 50, 69,  1, 52, 64, 70, 61, 53,  1, 30,  1, 53, 64,\n",
      "        21,  1,  1,  0,  1,  1,  0, 22, 63, 53,  1, 72, 57, 54, 63,  1, 72, 54,\n",
      "         1, 56, 64,  1, 55, 64, 67,  1, 50,  1, 72, 50, 61, 60,  1, 58, 63,  1,\n",
      "        69, 57, 54,  1, 65, 50, 67, 60,  1,  1,  0, 22, 63, 53,  1, 68, 57, 54,\n",
      "         1, 57, 64, 61, 53, 68,  1, 62, 54,  1, 50, 63, 53,  1, 68, 66, 70, 54,\n",
      "        54, 75, 54, 68,  1, 62, 74,  1, 57, 50, 63, 53,  1,  1,  0, 44, 54,  4,\n",
      "        61, 61,  1, 56, 64,  1, 64, 63,  1, 72, 50, 61, 60, 58, 63, 56,  1, 55,\n",
      "        64, 67,  1, 57, 64, 70, 67, 68,  1, 50, 63, 53,  1, 69, 50, 61, 60, 58,\n",
      "        63, 56,  1,  1,  0, 22, 51, 64, 70, 69,  1, 50, 61, 61,  1, 69, 57, 54,\n",
      "         1, 69, 57, 58, 63, 56, 68,  1, 69, 57, 50, 69,  1, 72, 54,  1, 65, 61,\n",
      "        50, 63,  1,  1,  0,  1,  1,  0, 40, 57, 54,  4, 68,  1, 59, 70, 68, 69,\n",
      "         1, 62, 74,  1, 60, 58, 63, 53,  1, 64, 55,  1, 56, 58, 67, 61,  7,  1,\n",
      "        68, 57, 54,  1, 62, 50, 60, 54, 68,  1, 62, 54,  1, 55, 54, 54, 61,  1,\n",
      "        55, 58, 63, 54,  1,  1,  0, 44, 57, 64,  1, 52, 64, 70, 61, 53,  1, 54,\n",
      "        71, 54, 67,  1, 51, 54, 61, 58, 54, 71, 54,  1, 69, 57, 50, 69,  1, 68,\n",
      "        57, 54,  1, 52, 64, 70, 61, 53,  1, 51, 54,  1, 62, 58, 63, 54, 21,  1,\n",
      "         1,  0, 40, 57, 54,  4, 68,  1, 59, 70, 68, 69,  1, 62, 74,  1, 60, 58,\n",
      "        63, 53,  1, 64, 55,  1, 56, 58, 67, 61,  7,  1, 72, 58, 69, 57, 64, 70,\n",
      "        69,  1, 57, 54, 67,  1, 30,  4, 62,  1, 51, 61, 70, 54,  1,  1,  0, 22,\n",
      "        63, 53,  1, 58, 55,  1, 68, 57, 54,  1, 54, 71, 54, 67,  1, 61, 54, 50,\n",
      "        71, 54, 68,  1, 62, 54,  1, 72, 57, 50, 69,  1, 52, 64, 70, 61, 53,  1,\n",
      "        30,  1, 53, 64,  7,  1, 72, 57, 50, 69,  1, 52, 64, 70, 61, 53,  1, 30,\n",
      "         1, 53, 64, 21,  0,  0,  3,  0,  3, 41, 50, 60, 54,  1, 58, 69,  1, 54,\n",
      "        50, 68, 74,  1, 72, 58, 69, 57,  1, 62, 54,  7,  1, 65, 61, 54, 50, 68,\n",
      "        54,  1,  1,  0, 41, 64, 70, 52, 57,  1, 62, 54,  1, 56, 54, 63, 69, 61,\n",
      "        74,  1, 61, 58, 60, 54,  1, 50,  1, 68, 70, 62, 62, 54, 67,  1, 54, 71,\n",
      "        54, 63, 58, 63, 56,  1, 51, 67, 54, 54, 75, 54,  1,  1,  0, 41, 50, 60,\n",
      "        54,  1, 74, 64, 70, 67,  1, 69, 58, 62, 54,  7,  1, 62, 50, 60, 54,  1,\n",
      "        58, 69,  1, 68, 61, 64, 72,  1,  1,  0, 22, 63, 53, 50, 63, 69, 54,  7,\n",
      "         1, 22, 63, 53, 50, 63, 69, 54,  1,  1,  0, 31, 70, 68, 69,  1, 61, 54,\n",
      "        69,  1, 69, 57, 54,  1, 55, 54, 54, 61, 58, 63, 56,  1, 56, 67, 64, 72,\n",
      "         1,  1,  0,  1,  1,  0, 34, 50, 60, 54,  1, 74, 64, 70, 67,  1, 55, 58,\n",
      "        63, 56, 54, 67, 68,  1, 68, 64, 55, 69,  1, 50, 63, 53,  1, 61, 58, 56,\n",
      "        57, 69,  1,  1,  0, 33, 54, 69,  1, 74, 64, 70, 67,  1, 51, 64, 53, 74,\n",
      "         1, 51, 54,  1, 69, 57, 54,  1, 71, 54, 61, 71, 54, 69,  1, 64, 55,  1,\n",
      "        69, 57, 54,  1, 63, 58, 56, 57, 69,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88deee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe2dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0a2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "# # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "# print('inputs:')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "\n",
    "# print('----')\n",
    "\n",
    "# for b in range(batch_size): # batch dimension\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850cc326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22d5952a350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(20834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7178ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) #BTC\n",
    "        q = self.query(x) #BTC\n",
    "        #compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # BTC BCT BTT // C**-0.5 is multiplied to preserve the variance before softmax aka scaling the attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) #BTT\n",
    "        wei = F.softmax(wei, dim=-1) #BTT\n",
    "        #to avoid overfitting dropout\n",
    "        wei = self.dropout(wei)\n",
    "        #weighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c7d3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2908533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e35d5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb265078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        #embedding for the respective word\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        #embedding for the position of that respective word\n",
    "        self.position_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        #using attention blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head = n_head) for _ in range(n_layers)]  \n",
    "        )\n",
    "        self.lnorm = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        #self attention\n",
    "#         self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        #feed-forward\n",
    "#         self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        #linear layer to get our logits\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        #idx and targets are both of B,T\n",
    "        tok_embd = self.token_embedding_table(idx)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
    "        X = tok_embd + pos_embd # B,T,C\n",
    "        X = self.blocks(X)\n",
    "        X = self.lnorm(X)\n",
    "#         X = self.sa_heads(X)\n",
    "#         X = self.ffwd(X)\n",
    "        logits = self.lm_head(X) # B,T,vocab_size\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "        #         print(logits.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "        #         print(targets.shape)\n",
    "            targets = targets.view(B*T)\n",
    "        #         print(logits.shape)\n",
    "        #         print(targets.shape)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            cond_idx = idx[:, -block_size:]\n",
    "            #get the predictions\n",
    "            logits, loss = self(cond_idx)\n",
    "            #focus on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            #softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "#             print(idx)\n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea9d9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4614, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "logits, loss = model(xb, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "845e45d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4963c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch optimizer Adamw\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "964ea9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 4.45646333694458\n",
      "Epoch : 501 Loss : 1.7578617334365845\n",
      "Epoch : 1001 Loss : 1.668043851852417\n",
      "Epoch : 1501 Loss : 1.4860448837280273\n",
      "Epoch : 2001 Loss : 1.4213839769363403\n",
      "Epoch : 2501 Loss : 1.405869960784912\n",
      "Epoch : 3001 Loss : 1.2972784042358398\n",
      "Epoch : 3501 Loss : 1.2759414911270142\n",
      "Epoch : 4001 Loss : 1.2694005966186523\n",
      "Epoch : 4501 Loss : 1.3592078685760498\n",
      "Epoch : 5001 Loss : 1.2475876808166504\n",
      "Epoch : 5501 Loss : 1.3143223524093628\n",
      "Epoch : 6001 Loss : 1.2377201318740845\n",
      "Epoch : 6501 Loss : 1.235092282295227\n",
      "Epoch : 7001 Loss : 1.1923811435699463\n",
      "Epoch : 7501 Loss : 1.1837154626846313\n",
      "Epoch : 8001 Loss : 1.2364931106567383\n",
      "Epoch : 8501 Loss : 1.2226368188858032\n",
      "Epoch : 9001 Loss : 1.2254959344863892\n",
      "Epoch : 9501 Loss : 1.2276448011398315\n",
      "1.2010911703109741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for steps in range(10000):\n",
    "    \n",
    "    #get a batch\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #calculate the loss and logits\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    #backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    #optimize\n",
    "    optimizer.step()\n",
    "    if steps%500 == 0:\n",
    "        print(f'Epoch : {steps+1} Loss : {loss.item()}')\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd8eb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.saveeee(model, './saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9498a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your wonder will smuh bells like the endertor vottation  \n",
      "There's no tearly calls me now  \n",
      "I could never be cooliday\n",
      "\n",
      "\"\n",
      "\"I'd almost steal watching the story and a phank wife  \n",
      "After the hillt.\n",
      "\n",
      "\"\n",
      "\"The silk of my brought  \n",
      "I'll know That my behind me  \n",
      "That's always gone and feel  \n",
      "And all your voice knows I want hard  \n",
      "All you want to tear  \n",
      "Have life the best of you  \n",
      "  \n",
      "Tonight  \n",
      "Have sayed  \n",
      "  \n",
      "I want to lift you up, boy  \n",
      "Have to let you do  \n",
      "Try to  \n",
      "float up, boy  \n",
      "Will my head shadows and my woman boy  \n",
      "Only you'll ever get me crazy house  \n",
      "Here comes the frage for a dratter baby  \n",
      "A world is allright then, a world that lives believe  \n",
      "A world that chenward friends, one way  \n",
      "Budgies free nevigr.  \n",
      "  \n",
      "One way of the Loud, Could  \n",
      "(One leaving David)  \n",
      "[ and lovers full knows  \n",
      "Galleludays as moonlight Co alive r-oh a blood orly]  \n",
      "  \n",
      "Give me it alright  \n",
      "Give me it all that's wrong!  \n",
      "It kissing it, It's broke me baby, then.  \n",
      "It fought my man, I'm giving in  \n",
      "She unlages the be\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype = torch.long, device=device), max_new_tokens = 1000)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94c83183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give a word to start generation : Motivate\n"
     ]
    }
   ],
   "source": [
    "request = input('Give a word to start generation : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "400bd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor(encode(list(request)), dtype = torch.long, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "026cc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae1f52fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motivate  \n",
      "  \n",
      "Motivate  \n",
      "I keeper  \n",
      "Swimmin' the bells  \n",
      "I keeper fallin' in fire  \n",
      "I know your song, yeah oh  \n",
      "You keep to hold into flesh  \n",
      "  \n",
      "Baby, just can't stop the sympathy's apart  \n",
      "Can I believe your love  \n",
      "  \n",
      "Believe your silent day with his dream  \n",
      "I love you down,  \n",
      "I love you down too far away  \n",
      "You wanna not me anyion or  \n",
      "I can't live your greenough  \n",
      "I'm gonna live yourself right inside you  \n",
      "Woman can't mean what armen, I'll mean what a give what I feel.  \n",
      "So I kiss your jakers to one yet stood, let your basket grown,  \n",
      "I'm abanzed on high and always fren  \n",
      "The snow of sight on her was b-shine, baby look up here in the day.  \n",
      "Baby, just a little voice to be your peek down road where lovers.  \n",
      "Wish you've never listened around  \n",
      "Shining down the Lady Dollady  \n",
      "I hear to crack the redeemade of trying alright that you was rainfor, ? Guess vit,  \n",
      "And it's trying to smoke.\n",
      "\n",
      "\"\n",
      "\"I'm saying it  \n",
      "I'm in aside of the sun  \n",
      "Yeah I  \n",
      "Everybody's oh so nut  \n",
      "It looks at you  \n",
      "  \n",
      "I could kn\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = index, max_new_tokens = 100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75e3cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23dac936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in this model : 10728268\n"
     ]
    }
   ],
   "source": [
    "print(f'Total parameters in this model : {total_parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "952edb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(76, 384)\n",
       "  (position_embedding_table): Embedding(76, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lnorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=76, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b6c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
